{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absent-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import spacy\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "classical-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "friendly-script",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29621, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "transsexual-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coastal-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impaired-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ['Claim', 'Premise']\n",
    "df = df.loc[(df['Component'].isin(valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning labels into two classes \n",
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'Claim':\n",
    "        classes.append(1.0)\n",
    "    else:\n",
    "        classes.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aggregate-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Annotation'] = classes\n",
    "df.Annotation.value_counts()\n",
    "df = df[['Speech', 'Annotation', 'Set']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "expanded-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_connectives (df, speech_sents):\n",
    "\n",
    "    \"\"\" \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: numpy array of text data instances in DataFrame\n",
    "    :return: df: DataFrame with a new feature Claim_Connective, \n",
    "            representing the presence/absence of any connective from a given list in a sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    connectives = ['so that', 'as a result', 'therefore', 'thus', 'thereby', 'in the end', 'hence', 'accordingly', 'in this way']\n",
    "    lst = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in connectives):\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    df['Claim_Connective'] = lst\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "comic-richmond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Set</th>\n",
       "      <th>Claim_Connective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and after 911 it became clear that we had to d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and we also then finally had to stand up democ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what we did in iraq was exactly the right thin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>if i had it to recommend all over again i woul...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the world is far safer today because saddam hu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29615</th>\n",
       "      <td>but our longterm security depends on our deep ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29616</th>\n",
       "      <td>and well continue to promote freedom around th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29617</th>\n",
       "      <td>freedom is on the march</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29618</th>\n",
       "      <td>tomorrow afghanistan will be voting for a pres...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>in iraq well be having free elections and a fr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Speech  Annotation  \\\n",
       "3      and after 911 it became clear that we had to d...         1.0   \n",
       "4      and we also then finally had to stand up democ...         0.0   \n",
       "9      what we did in iraq was exactly the right thin...         1.0   \n",
       "10     if i had it to recommend all over again i woul...         0.0   \n",
       "11     the world is far safer today because saddam hu...         0.0   \n",
       "...                                                  ...         ...   \n",
       "29615  but our longterm security depends on our deep ...         1.0   \n",
       "29616  and well continue to promote freedom around th...         1.0   \n",
       "29617                            freedom is on the march         1.0   \n",
       "29618  tomorrow afghanistan will be voting for a pres...         0.0   \n",
       "29619  in iraq well be having free elections and a fr...         0.0   \n",
       "\n",
       "              Set  Claim_Connective  \n",
       "3           TRAIN                 0  \n",
       "4           TRAIN                 0  \n",
       "9           TRAIN                 0  \n",
       "10          TRAIN                 0  \n",
       "11          TRAIN                 0  \n",
       "...           ...               ...  \n",
       "29615  VALIDATION                 0  \n",
       "29616  VALIDATION                 0  \n",
       "29617  VALIDATION                 0  \n",
       "29618  VALIDATION                 0  \n",
       "29619  VALIDATION                 0  \n",
       "\n",
       "[22280 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_connectives(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indirect-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment (df, speech_sents): \n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        vs = analyzer.polarity_scores(sent)\n",
    "        senti.append([list(vs.values())[3]])\n",
    "    \n",
    "    senti_arr = np.array(senti)\n",
    "    df['Sentiment'] = senti_arr\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vertical-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Set</th>\n",
       "      <th>Claim_Connective</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and after 911 it became clear that we had to d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and we also then finally had to stand up democ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.7721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what we did in iraq was exactly the right thin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>if i had it to recommend all over again i woul...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the world is far safer today because saddam hu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29615</th>\n",
       "      <td>but our longterm security depends on our deep ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29616</th>\n",
       "      <td>and well continue to promote freedom around th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29617</th>\n",
       "      <td>freedom is on the march</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29618</th>\n",
       "      <td>tomorrow afghanistan will be voting for a pres...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>in iraq well be having free elections and a fr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VALIDATION</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Speech  Annotation  \\\n",
       "3      and after 911 it became clear that we had to d...         1.0   \n",
       "4      and we also then finally had to stand up democ...         0.0   \n",
       "9      what we did in iraq was exactly the right thin...         1.0   \n",
       "10     if i had it to recommend all over again i woul...         0.0   \n",
       "11     the world is far safer today because saddam hu...         0.0   \n",
       "...                                                  ...         ...   \n",
       "29615  but our longterm security depends on our deep ...         1.0   \n",
       "29616  and well continue to promote freedom around th...         1.0   \n",
       "29617                            freedom is on the march         1.0   \n",
       "29618  tomorrow afghanistan will be voting for a pres...         0.0   \n",
       "29619  in iraq well be having free elections and a fr...         0.0   \n",
       "\n",
       "              Set  Claim_Connective  Sentiment  \n",
       "3           TRAIN                 0    -0.7269  \n",
       "4           TRAIN                 0    -0.7721  \n",
       "9           TRAIN                 0     0.0000  \n",
       "10          TRAIN                 0     0.6124  \n",
       "11          TRAIN                 0     0.1531  \n",
       "...           ...               ...        ...  \n",
       "29615  VALIDATION                 0     0.9081  \n",
       "29616  VALIDATION                 0     0.8360  \n",
       "29617  VALIDATION                 0     0.6369  \n",
       "29618  VALIDATION                 0     0.0000  \n",
       "29619  VALIDATION                 0     0.9041  \n",
       "\n",
       "[22280 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_sentiment(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tested-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_loaded = spacy.load(\"en_core_web_sm\")\n",
    "# tag text and extract tags into a list\n",
    "\n",
    "df['ner'] = df['Speech'].apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in spacy_loaded(x).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "single-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# utils function to count the element of a list\n",
    "\n",
    "def utils_lst_count(lst):\n",
    "    dic_counter = collections.Counter()\n",
    "    \n",
    "    for x in lst:\n",
    "        dic_counter[x] += 1\n",
    "    \n",
    "    dic_counter = collections.OrderedDict( \n",
    "                     sorted(dic_counter.items(), \n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    lst_count = [{key:value} for key,value in dic_counter.items()]\n",
    "    \n",
    "    return lst_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "black-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tags\n",
    "df['ner'] = df['ner'].apply(lambda x: utils_lst_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subject-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils function create new column for each tag category\n",
    "\n",
    "def utils_ner_features(lst_dics_tuples, tag):\n",
    "    if len(lst_dics_tuples) > 0:\n",
    "        tag_type = []\n",
    "        for dic_tuples in lst_dics_tuples:\n",
    "            for tuple in dic_tuples:\n",
    "                type, n = tuple[1], dic_tuples[tuple]\n",
    "                tag_type = tag_type + [type]*n\n",
    "                dic_counter = collections.Counter()\n",
    "                for x in tag_type:\n",
    "                    dic_counter[x] += 1\n",
    "        return dic_counter[tag]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stainless-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "\n",
    "tags_set = []\n",
    "\n",
    "for lst in df['ner'].tolist():\n",
    "    for dic in lst:\n",
    "        for k in dic.keys():\n",
    "            tags_set.append(k[1])\n",
    "            \n",
    "tags_set = list(set(tags_set))\n",
    "\n",
    "for feature in tags_set:\n",
    "    df['ner_' + feature] = df['ner'].apply(lambda x: utils_ner_features(x, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "miniature-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ner'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loving-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos'] = df['Speech'].apply(lambda x: [(tag.text, tag.pos_) \n",
    "                                for tag in spacy_loaded(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "boring-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tags\n",
    "df['pos'] = df['pos'].apply(lambda x: utils_lst_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "heated-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pos \n",
    "pos_set = []\n",
    "\n",
    "for lst in df['pos'].tolist():\n",
    "    for dic in lst:\n",
    "        for k in dic.keys():\n",
    "            pos_set.append(k[1])\n",
    "            \n",
    "pos_set = list(set(pos_set))\n",
    "\n",
    "for feature in pos_set:\n",
    "    df['pos_' + feature] = df['pos'].apply(lambda x: utils_ner_features(x, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "compressed-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only adverbs and adjectives and dropping other pos, like authors had\n",
    "for feature in df.columns:\n",
    "    if feature != 'pos_ADV' and feature != 'pos_ADJ' and 'pos' in feature:\n",
    "        df = df.drop(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "packed-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting as the authors did \n",
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "particular-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Set'], axis=1)\n",
    "df_test = df_test.drop(['Set'], axis=1)\n",
    "\n",
    "X_train = df_train.drop(['Annotation'], axis=1)\n",
    "y_train = df_train.Annotation\n",
    "X_test = df_test.drop(['Annotation'], axis=1)\n",
    "y_test = df_test.Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "worth-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "bow_train = bow.fit_transform(X_train['Speech'])\n",
    "bow_test = bow.transform(X_test['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "upper-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = bow.get_feature_names()\n",
    "dense = bow_train.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "overhead-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "distinguished-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.hstack([X_train, fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "composite-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10022\n",
      "(10464, 10022)\n",
      "[[ 0.     -0.7269  0.     ...  0.      0.      0.    ]\n",
      " [ 0.     -0.7721  0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " ...\n",
      " [ 0.     -0.5994  0.     ...  0.      0.      0.    ]\n",
      " [ 0.     -0.296   0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.5106  0.     ...  0.      0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "in_dim = train_features.shape[1]\n",
    "print(in_dim)\n",
    "print(train_features.shape)\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "toxic-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = bow.get_feature_names()\n",
    "dense = bow_test.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "asian-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "atomic-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.hstack([X_test, fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "thick-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 10022)             100450506 \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                641472    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,094,091\n",
      "Trainable params: 101,094,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#keras NN model initialization\n",
    "\n",
    "model = keras.Sequential([\n",
    "    #input layer\n",
    "    keras.layers.Dense(in_dim,input_shape=(in_dim,)),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(32, activation='sigmoid'),\n",
    "    #output layer\n",
    "    keras.layers.Dense(1, activation='sigmoid'),   \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ready-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F893E8AF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F893E8AF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1308/1308 [==============================] - 494s 377ms/step - loss: 0.6153 - auc: 0.7168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f893f884c8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = np.asarray(train_features)\n",
    "y_train = np.asarray(y_train)    \n",
    "model.fit(train_features, y_train, epochs=1, batch_size=8)\n",
    "model.fit(train_features, y_train, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "limiting-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001F893FC05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001F893FC05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_features)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "musical-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Premise      0.635     0.613     0.624      3214\n",
      "       Claim      0.642     0.664     0.653      3361\n",
      "\n",
      "    accuracy                          0.639      6575\n",
      "   macro avg      0.639     0.638     0.638      6575\n",
      "weighted avg      0.639     0.639     0.639      6575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Premise', 'Claim']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
